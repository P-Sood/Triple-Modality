# The Three Musketeers: Audio, Text, and Video

## Data
We use MELD, IEMOCAP, Mustard, and SexTok as datasets for this project.

## General
This project aims to detect emotions from multiple sources of information, including text, audio, and facial expressions. While previous studies have primarily focused on text or images, my research explores how combining multiple modalities can improve the accuracy and robustness of emotion recognition.

I have developed a new method for selecting the most relevant features from audio and video data, as well as a new approach for training a neural network that can fuse information from all three modalities. My initial tests on English datasets have shown that using text and audio together yields better results than using them separately. However, adding video data does not significantly improve performance, suggesting that it may contain more noise or irrelevant information than audio or text data.

Moving forward, I plan to continue exploring ways to improve the performance of the video modality and to investigate the optimal method for combining all three models. Stay tuned for more updates on this exciting project!
