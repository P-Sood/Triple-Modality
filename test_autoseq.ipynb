{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForSequenceClassification \n",
    "import torch\n",
    "from typing import OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def way1():\n",
    "    ckpt = torch.load(\"results/IEMOCAP/roberta-large/final/2021-05-09-12-19-54-speaker_mode-upper-num_past_utterances-1000-num_future_utterances-0-batch_size-4-seed-4/checkpoint-5975/pytorch_model.bin\",map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    bert = AutoModel.from_pretrained(\"roberta-large\")\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in ckpt.items():\n",
    "        name = k.replace(\"roberta.\" , \"\")  # remove 'roberta.' prefix\n",
    "        \n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load the new state dictionary into your model\n",
    "    bert.load_state_dict(new_state_dict , strict = False)\n",
    "    # bert.load_state_dict(ckpt)\n",
    "    # print(bert.classifier.out_proj.bias , flush = True)\n",
    "\n",
    "\n",
    "    # Zero out all gradients. Might not need this anymore though\n",
    "    # for i, model in enumerate([bert, whisper, videomae]):\n",
    "    for i, model in enumerate([bert]):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Put the models onto eval mode\n",
    "    bert.eval()\n",
    "    return bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = way1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def way2():\n",
    "    ckpt = torch.load(\"results/IEMOCAP/roberta-large/final/2021-05-09-12-19-54-speaker_mode-upper-num_past_utterances-1000-num_future_utterances-0-batch_size-4-seed-4/checkpoint-5975/pytorch_model.bin\",map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    bert = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\" , num_labels = 6)\n",
    "    \n",
    "    # Load the new state dictionary into your model\n",
    "    bert.load_state_dict(ckpt , strict = False)\n",
    "    # bert.load_state_dict(ckpt)\n",
    "    # print(bert.classifier.out_proj.bias , flush = True)\n",
    "\n",
    "\n",
    "    # Zero out all gradients. Might not need this anymore though\n",
    "    # for i, model in enumerate([bert, whisper, videomae]):\n",
    "    for i, model in enumerate([bert]):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Put the models onto eval mode\n",
    "    bert.eval()\n",
    "    return bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "c = way2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(1 , 1000 ,(1 , 512))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b(x)\n",
    "last_hidden_states = b(x)['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = c(x , output_hidden_states=True)['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3282, -0.1843,  0.3596,  ..., -0.0457, -0.0643, -0.1438],\n",
       "          [-0.2051, -0.2664,  0.1925,  ...,  0.0569, -0.0028, -0.1785],\n",
       "          [-0.7907, -0.5648,  0.3171,  ..., -0.2565,  0.5877, -0.1158],\n",
       "          ...,\n",
       "          [-0.3575, -0.8840,  0.1654,  ...,  0.6791,  0.7535,  0.1315],\n",
       "          [-0.2939, -0.2007,  0.3281,  ...,  0.7667,  0.8214, -0.4741],\n",
       "          [-0.2771, -0.3241,  0.5546,  ...,  0.2184, -0.2696, -0.1907]]]),\n",
       " torch.Size([1, 512, 1024]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states , last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2685, -0.8295,  0.3979,  ..., -0.5905, -0.4159, -0.6246],\n",
       "          [-0.0218, -0.0437,  0.0613,  ..., -0.3169,  0.3718, -0.4067],\n",
       "          [ 1.7885, -1.2825, -0.4770,  ..., -1.7930,  0.4633, -1.0925],\n",
       "          ...,\n",
       "          [ 0.2500,  0.6878,  1.1036,  ..., -0.6367, -0.6602, -0.2775],\n",
       "          [ 0.1526, -0.4661,  0.0468,  ...,  0.8816,  0.3922,  0.1328],\n",
       "          [-0.3057,  1.3320,  1.4506,  ...,  0.8307, -0.6729, -0.7277]]]),\n",
       " torch.Size([1, 512, 1024]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0], hidden_states[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "featuregen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
