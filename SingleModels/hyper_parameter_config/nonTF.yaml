program: ../text_nn.py
command:
  - ${env}
  - python3
  - ${program}
  - "--dataset"
  - "../../data/meld"
method: bayes

metric:
  goal: maximize
  name: val/weighted-f1-score
parameters:
  epoch:
    max: 12
    min: 6
    distribution: int_uniform
  learning_rate:
    values: [1e-6 , 1e-5 , 1e-4  , 5e-5 , 5e-6]
  weight_decay:
    values: [1e-4, 1e-3, 1e-2]
  batch_size:
    values: [8 , 16 , 32 , 64,]
  seed:
    values: [32, 64, 96]
  mask:
    values: [False]
  early_div:
    values: [False]
  learn_PosEmbeddings:
    values: [False]
  num_layers:
    values: [3, 4, 5, 6, 7, 8]
  patience:
    values: [14]
  clip:
    values: [0.1 , 1 , 5]
  epoch_switch:
    values: [2 , 3]
  dropout:
    values: [ 0.2 , 0.3 , 0.4 , 0.5 ]
  T_max:
   values: [2 , 3]
  hidden_size:
    values: [64, 128, 256, 384, 768, 1536]
  label_task:
    values: ['emotion']
  sota:
    values: [True, False]
  model:
    values: ['MAE_encoder']
  loss:
    values: ['NewCrossEntropy' , 'CrossEntropy' , 'WeightedCrossEntropy']
  sampler:
    values: ['Both' , 'Iterative' , 'Weighted']
  BertModel:
    values: ['j-hartmann/emotion-english-roberta-large' , 'tasinhoque/roberta-large-go-emotions-2' , 'roberta-large']
  text_column:
    values: ['all_dialog_text']
  beta:
    values: [1]
