program: ../text_nn.py
command:
  - ${env}
  - python3
  - ${program}
  - "--dataset"
  - "../../data/meld"
method: bayes

metric:
  goal: maximize
  name: val/weighted-f1-score
parameters:
  epoch:
    max: 12
    min: 6
    distribution: int_uniform
  learning_rate:
    max: 9e-4
    min: 1e-6
    distribution: uniform
  weight_decay:
    max: 1e-2
    min: 1e-4
    distribution: uniform
  batch_size:
    values: [8 , 16 , 32]
  seed:
    values: [32, 64, 96]
  mask:
    values: [False]
  early_div:
    values: [False]
  learn_PosEmbeddings:
    values: [False]
  num_layers:
    values: [3, 4, 5, 6, 7, 8]
  patience:
    values: [14]
  clip:
    values: [1 , 5]
  epoch_switch:
    values: [2 , 3]
  dropout:
    values: [ 0.2 , 0.3 , 0.4 , 0.5 ]
  T_max:
   values: [2 , 3]
  hidden_size:
    values: [64, 128, 256, 384, 768, 1536]
  label_task:
    values: ['emotion']
  sota:
    values: [True, False]
  model:
    values: ['MAE_encoder']
  loss:
    values: ['NewCrossEntropy' , 'CrossEntropy' , 'WeightedCrossEntropy']
  sampler:
    values: ['Both' , 'Iterative' , 'Weighted']
  BertModel:
    values: ['arpanghoshal/EmoRoBERTa' , 'tae898/emoberta-large' , 'j-hartmann/emotion-english-roberta-large']
  beta:
    values: [1]
